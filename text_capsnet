# coding=utf-8
import text_layer as text_layer
import os
import numpy as np
import keras.backend as K
from keras.optimizers import Adam
from keras.utils import to_categorical
from callbacks import HyperbolicTangentLR
from keras.callbacks import ModelCheckpoint
from keras.callbacks import Callback
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, recall_score, precision_score


class Metrics(Callback):
    def __init__(self, valid_data):
        super(Metrics, self).__init__()
        self.validation_data = valid_data

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)
        val_targ = self.validation_data[1]
        if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:
            val_targ = np.argmax(val_targ, -1)

        _val_f1 = f1_score(val_targ, val_predict, average='macro')
        _val_recall = recall_score(val_targ, val_predict, average='macro')
        _val_precision = precision_score(val_targ, val_predict, average='macro')

        logs['val_f1'] = _val_f1
        logs['val_recall'] = _val_recall
        logs['val_precision'] = _val_precision
        print(" - val_f1:{:.3f} - val_precision:{:.3f} - val_recall:{:.3f}".format(_val_f1, _val_precision, _val_recall))
        return
    pass


class TextCapsnet(object):
    def __init__(self, args,
                 seq_len=600,
                 num_classes=2,
                 vocab_size=30000,
                 x_train=None,
                 y_train=None,
                 x_test=None,
                 y_test=None,
                 pretrain_vec=None):
        self.args = args
        self.init_lr = args.init_lr
        self.batch_size = args.batch_size
        self.epochs = args.epochs
        self.l2 = args.l2
        self.routing = args.routing
        self.embedding_size = args.embedding_size
        self.dropout_ratio = args.dropout_ratio
        self.num_filter = args.num_filter
        self.filter_size = args.filter_size

        self.num_capsule = args.num_cap
        self.len_ui = args.len_ui
        self.len_vj = args.len_vj

        self.num_classes = num_classes
        self.sequence_length = seq_len
        self.vocab_size = vocab_size

        self.x_train = x_train
        self.y_train = y_train
        self.x_test = x_test
        self.y_test = y_test

        self.pretrain_vec = pretrain_vec

    def train(self):
        model = text_layer.get_model(self, summary=True)

        # callbacks
        filepath = os.path.join(self.args.save_ckpt, "{}_checkpoint.h5".format('Twitter'))

        lr_scheduler = HyperbolicTangentLR(init_lr=self.init_lr, max_epoch=self.epochs, L=-6, U=3)
        ckpt_callback = ModelCheckpoint(filepath,
                                        monitor='val_acc',
                                        save_best_only=True, save_weights_only=True, mode='max')

        # train
        model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs,
                  validation_data=[self.x_test, self.y_test],
                  callbacks=[Metrics(valid_data=(self.x_test, self.y_test)), lr_scheduler, ckpt_callback])

        # model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.epochs,
        #          validation_data=[self.x_test, self.y_test])
        model.load_weights(filepath)
        test_loss, test_acc = model.evaluate(self.x_test, self.y_test)
        print("TEST ACC : {:.3f}".format(test_acc))


